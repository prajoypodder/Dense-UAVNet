{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install scikit-plot\n",
        "%pip install tf-keras-vis tensorflow"
      ],
      "metadata": {
        "id": "aRSaxT5sKYyd"
      },
      "id": "aRSaxT5sKYyd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tqdm\n",
        "%pip install opencv-python\n",
        "%pip install seaborn"
      ],
      "metadata": {
        "id": "cXmIbbUJKcsJ"
      },
      "id": "cXmIbbUJKcsJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from matplotlib import cm"
      ],
      "metadata": {
        "id": "dSB6HL1bKgwr"
      },
      "id": "dSB6HL1bKgwr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "027650c8-7f5c-4ace-9d6f-515a96cdb153",
      "metadata": {
        "id": "027650c8-7f5c-4ace-9d6f-515a96cdb153"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import cv2\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir = './Outdoor Data'\n",
        "def Read_Images_and_Make_Numpy_Files():\n",
        "  images = []\n",
        "  labels = []\n",
        "  classes = os.listdir(dataDir)\n",
        "  print('Classes: {}'.format(classes))\n",
        "  for class_ in classes:\n",
        "    if class_ == '0: Type 2':\n",
        "      print('Reading Data for {} class'.format(class_))\n",
        "      imageNames = os.listdir(os.path.join(dataDir, class_))\n",
        "      for imgName in tqdm(imageNames):\n",
        "        img = load_img(os.path.join(dataDir, class_, imgName), target_size=(224, 224), color_mode='rgb', interpolation='lanczos')\n",
        "        img_array = img_to_array(img, data_format='channels_last', dtype='float32')\n",
        "        images.append(img_array)\n",
        "        labels.append(0)\n",
        "    elif class_ == '1: Type 3':\n",
        "      print('Reading Data for {} class'.format(class_))\n",
        "      imageNames = os.listdir(os.path.join(dataDir, class_))\n",
        "      for imgName in tqdm(imageNames):\n",
        "        img = load_img(os.path.join(dataDir, class_, imgName), target_size=(224, 224), color_mode='rgb', interpolation='lanczos')\n",
        "        img_array = img_to_array(img, data_format='channels_last', dtype='float32')\n",
        "        images.append(img_array)\n",
        "        labels.append(1)\n",
        "    elif class_ == '2: Type 4':\n",
        "      print('Reading Data for {} class'.format(class_))\n",
        "      imageNames = os.listdir(os.path.join(dataDir, class_))\n",
        "      for imgName in tqdm(imageNames):\n",
        "        img = load_img(os.path.join(dataDir, class_, imgName), target_size=(224, 224), color_mode='rgb', interpolation='lanczos')\n",
        "        img_array = img_to_array(img, data_format='channels_last', dtype='float32')\n",
        "        images.append(img_array)\n",
        "        labels.append(2)\n",
        "    elif class_ == '3: Type 5':\n",
        "      print('Reading Data for {} class'.format(class_))\n",
        "      imageNames = os.listdir(os.path.join(dataDir, class_))\n",
        "      for imgName in tqdm(imageNames):\n",
        "        img = load_img(os.path.join(dataDir, class_, imgName), target_size=(224, 224), color_mode='rgb', interpolation='lanczos')\n",
        "        img_array = img_to_array(img, data_format='channels_last', dtype='float32')\n",
        "        images.append(img_array)\n",
        "        labels.append(3)\n",
        "    elif class_ == '4: Type 6':\n",
        "      print('Reading Data for {} class'.format(class_))\n",
        "      imageNames = os.listdir(os.path.join(dataDir, class_))\n",
        "      for imgName in tqdm(imageNames):\n",
        "        img = load_img(os.path.join(dataDir, class_, imgName), target_size=(224, 224), color_mode='rgb', interpolation='lanczos')\n",
        "        img_array = img_to_array(img, data_format='channels_last', dtype='float32')\n",
        "        images.append(img_array)\n",
        "        labels.append(4)\n",
        "    elif class_ == '5: Type 7':\n",
        "      print('Reading Data for {} class'.format(class_))\n",
        "      imageNames = os.listdir(os.path.join(dataDir, class_))\n",
        "      for imgName in tqdm(imageNames):\n",
        "        img = load_img(os.path.join(dataDir, class_, imgName), target_size=(224, 224), color_mode='rgb', interpolation='lanczos')\n",
        "        img_array = img_to_array(img, data_format='channels_last', dtype='float32')\n",
        "        images.append(img_array)\n",
        "        labels.append(5)\n",
        "\n",
        "\n",
        "  images = np.array(images).reshape(-1, 224, 224, 3)\n",
        "  print('Images: {}'.format(images.shape))\n",
        "  labels = np.array(labels)\n",
        "  print('Labels: {}'.format(labels.shape))\n",
        "  np.save('./data output environment/images', images)\n",
        "  np.save('./data output environment/labels', labels)\n",
        "\n",
        "\n",
        "Read_Images_and_Make_Numpy_Files()"
      ],
      "metadata": {
        "id": "d0_IV9K2lWSV"
      },
      "id": "d0_IV9K2lWSV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Load the data\n",
        "dataDir = './data output environment'\n",
        "x = np.load(dataDir + '/images.npy')\n",
        "y = np.load(dataDir + '/labels.npy')\n",
        "\n",
        "print('Images: {} | Labels: {}'.format(x.shape, y.shape))"
      ],
      "metadata": {
        "id": "H4hnvHSsKsDP"
      },
      "id": "H4hnvHSsKsDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d418b5-a184-45de-94dd-d1260ee09fb2",
      "metadata": {
        "id": "f5d418b5-a184-45de-94dd-d1260ee09fb2"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20, random_state=2020)\n",
        "\n",
        "print('Training Images: {} | Test Images: {}'.format(train_x.shape, test_x.shape))\n",
        "print('Training Labels: {} | Test Labels: {}'.format(train_y.shape, test_y.shape))\n",
        "\n",
        "\n",
        "print('Train: {} , {} | Test: {} , {}'.format(train_x.min(), train_x.max(), test_x.min(), test_x.max()))\n",
        "\n",
        "train_x /= 255.0\n",
        "test_x /= 255.0\n",
        "\n",
        "print('Train: {} , {} | Test: {} , {}'.format(train_x.min(), train_x.max(), test_x.min(), test_x.max()))\n",
        "\n",
        "\n",
        "print('0_ Type 2 | 1_ Type 3 | 2_ Type 4 | 3_ Type 5 | 4_ Type 6 | 5_ Type 7')\n",
        "\n",
        "\n",
        "print(Counter(train_y))\n",
        "\n",
        "\n",
        "print(Counter(test_y))\n",
        "\n",
        "\n",
        "train_y_one_hot = tf.one_hot(train_y, depth=6)\n",
        "test_y_one_hot = tf.one_hot(test_y, depth=6)\n",
        "\n",
        "print('Training Labels: {} | Test Labels: {}'.format(train_y_one_hot.shape, test_y_one_hot.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abc2128-ad5c-4d97-bbf3-29694ffca8bd",
      "metadata": {
        "id": "5abc2128-ad5c-4d97-bbf3-29694ffca8bd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Define the input shape (assuming train_x is already defined)\n",
        "input_shape = train_x.shape[1:]\n",
        "\n",
        "# Function to create a DenseNet model with additional dense layers and dropout\n",
        "def create_densenet_model(base_model, input_shape, num_classes):\n",
        "    # Load base DenseNet model\n",
        "    base = base_model(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Add custom pooling and dense layers\n",
        "    x = GlobalAveragePooling2D()(base.output)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=base.input, outputs=output_layer)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 6\n",
        "\n",
        "# Class names\n",
        "classes = ['Type 2', 'Type 3', 'Type 4', 'Type 5', 'Type 6', 'Type 7']\n",
        "\n",
        "# DenseNet variants\n",
        "densenet_variants = {\n",
        "    \"DenseNet121\": DenseNet121,\n",
        "    \"DenseNet169\": DenseNet169,\n",
        "    \"DenseNet201\": DenseNet201,\n",
        "}\n",
        "\n",
        "# Train and evaluate each DenseNet variant\n",
        "for name, densenet in densenet_variants.items():\n",
        "    print(f\"\\nTraining model: {name}\")\n",
        "    model = create_densenet_model(densenet, input_shape, num_classes)\n",
        "\n",
        "    # Display the model summary\n",
        "    print(f\"\\n{name} - Model Summary\")\n",
        "    #model.summary()\n",
        "\n",
        "    # Define the callbacks\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
        "\n",
        "    # Train the model with callbacks\n",
        "    history = model.fit(train_x, train_y, epochs=25, batch_size=32, validation_split=0.2,\n",
        "                        callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "    # Plot accuracy and loss curves\n",
        "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'{name} - Epoch vs. Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history.history['loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{name} - Epoch vs. Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate model on test set\n",
        "    y_pred = np.argmax(model.predict(test_x), axis=1)\n",
        "    y_true = test_y  # Assuming test_y has the true labels\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(f'{name} - Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate FPR, FNR, TPR, TNR for each class\n",
        "    metrics = {}\n",
        "    for i, cls in enumerate(classes):\n",
        "        TP = cm[i, i]\n",
        "        FN = cm[i, :].sum() - TP\n",
        "        FP = cm[:, i].sum() - TP\n",
        "        TN = cm.sum() - (TP + FN + FP)\n",
        "\n",
        "        TPR = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        FNR = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        TNR = TN / (FP + TN) if (FP + TN) > 0 else 0\n",
        "\n",
        "        metrics[cls] = {\"TPR\": TPR, \"FNR\": FNR, \"FPR\": FPR, \"TNR\": TNR}\n",
        "\n",
        "    # Display metrics as a DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics).transpose()\n",
        "    print(f'{name} - FPR, FNR, TPR, TNR')\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    print(f'{name} - Classification Report')\n",
        "    print(report_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NADAM Optimizer\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = train_x.shape[1:]\n",
        "\n",
        "# Function to create a DenseNet model with additional dense layers and dropout\n",
        "def create_densenet_model(base_model, input_shape, num_classes, optimizer):\n",
        "    # Load base DenseNet model\n",
        "    base = base_model(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Add custom pooling and dense layers\n",
        "    x = GlobalAveragePooling2D()(base.output)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=base.input, outputs=output_layer)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 6\n",
        "\n",
        "# Class names\n",
        "classes = ['Type 2', 'Type 3', 'Type 4', 'Type 5', 'Type 6', 'Type 7']\n",
        "\n",
        "# DenseNet variants\n",
        "densenet_variants = {\n",
        "    \"DenseNet169\": DenseNet169,\n",
        "    \"DenseNet201\": DenseNet201,\n",
        "    \"DenseNet121\": DenseNet121,\n",
        "}\n",
        "\n",
        "# Train and evaluate each DenseNet variant with Nadam optimizer\n",
        "for name, densenet in densenet_variants.items():\n",
        "    print(f\"\\nTraining model: {name}\")\n",
        "    optimizer = Nadam(learning_rate=0.0001)\n",
        "    model = create_densenet_model(densenet, input_shape, num_classes, optimizer)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_x, train_y, epochs=25, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Plot accuracy and loss curves\n",
        "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'{name} - Epoch vs. Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history.history['loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{name} - Epoch vs. Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate model on test set\n",
        "    y_pred = np.argmax(model.predict(test_x), axis=1)\n",
        "    y_true = test_y  # Assuming test_y has the true labels\n",
        "\n",
        "    # Confusion matrix (tabular format)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "    print(f'\\n{name} - Confusion Matrix (Tabular Format):')\n",
        "    print(cm_df)\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    print(f'\\n{name} - Classification Report:')\n",
        "    print(report_df)\n"
      ],
      "metadata": {
        "id": "jd9xJtpEjYw4"
      },
      "id": "jd9xJtpEjYw4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tf_cpu]",
      "language": "python",
      "name": "conda-env-tf_cpu-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}